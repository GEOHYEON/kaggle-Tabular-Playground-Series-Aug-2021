{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7a4ef4",
   "metadata": {},
   "source": [
    "# TabNet CODE\n",
    "\n",
    "### Introduce TabNET\n",
    "---\n",
    "* 고스트 배치 정규화 (GBN)\n",
    "\n",
    "* Sparsemax\n",
    "\n",
    "* \n",
    "\n",
    "\n",
    "### code_source\n",
    "---\n",
    "* TabNet Torch code: https://ichi.pro/ko/pytorcheseo-tabnet-guhyeon-277727554318969\n",
    "* Sparsemax code: https://github.com/gokceneraslan/SparseMax.torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3890dab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.11 (default, Aug  3 2021, 05:10:14) \\n[Clang 10.0.0 ]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad7e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bc66e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import optuna\n",
    "\n",
    "import plotly as pl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dbaaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBN(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp,vbs=128,momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "        \n",
    "    def forward(self,x):\n",
    "        chunk = torch.chunk(x,x.size(0)//self.vbs,0)\n",
    "        res = [self.bn(y) for y in chunk]\n",
    "        return torch.cat(res,0)\n",
    "\n",
    "# Attention Transformer\n",
    "class AttentionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_a,inp_dim,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_a,inp_dim)\n",
    "        self.bn = GBN(out_dim,vbs=vbs)\n",
    "        self.smax = Sparsemax()\n",
    "        self.r = relax\n",
    "        \n",
    "    #a:feature from previous decision step\n",
    "    def forward(self,a,priors): \n",
    "        a = self.bn(self.fc(a)) \n",
    "        mask = self.smax(a*priors) \n",
    "        priors =priors*(self.r-mask)  #updating the prior\n",
    "        return mask\n",
    "    \n",
    "# \n",
    "class GLU(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n",
    "        super().__init__()\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = nn.Linear(inp_dim,out_dim*2)\n",
    "        self.bn = GBN(out_dim*2,vbs=vbs) \n",
    "        self.od = out_dim\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn(self.fc(x))\n",
    "        return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n",
    "        super().__init__()\n",
    "        first = True\n",
    "        self.shared = nn.ModuleList()\n",
    "        if shared:\n",
    "            self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n",
    "            first= False    \n",
    "            for fc in shared[1:]:\n",
    "                self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n",
    "        else:\n",
    "            self.shared = None\n",
    "        self.independ = nn.ModuleList()\n",
    "        if first:\n",
    "            self.independ.append(GLU(inp,out_dim,vbs=vbs))\n",
    "        for x in range(first, n_ind):\n",
    "            self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n",
    "        self.scale = torch.sqrt(torch.tensor([.5],device=device))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.shared:\n",
    "            x = self.shared[0](x)\n",
    "            for glu in self.shared[1:]:\n",
    "                x = torch.add(x, glu(x))\n",
    "                x = x*self.scale\n",
    "        for glu in self.independ:\n",
    "            x = torch.add(x, glu(x))\n",
    "            x = x*self.scale\n",
    "        return x\n",
    "\n",
    "class DecisionStep(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n",
    "        self.atten_tran =  AttentionTransformer(n_a,inp_dim,relax,vbs)\n",
    "        \n",
    "    def forward(self,x,a,priors):\n",
    "        mask = self.atten_tran(a,priors)\n",
    "        sparse_loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n",
    "        x = self.fea_tran(x*mask)\n",
    "        return x,sparse_loss\n",
    "\n",
    "class TabNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,\n",
    "                 n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n",
    "        super().__init__()\n",
    "        if n_shared>0:\n",
    "            self.shared = nn.ModuleList()\n",
    "            self.shared.append(nn.Linear(inp_dim,2*(n_d+n_a)))\n",
    "            for x in range(n_shared-1):\n",
    "                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n",
    "        else:\n",
    "            self.shared=None\n",
    "        self.first_step = FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind) \n",
    "        self.steps = nn.ModuleList()\n",
    "        for x in range(n_steps-1):\n",
    "            self.steps.append(DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n",
    "        self.fc = nn.Linear(n_d,final_out_dim)\n",
    "        self.bn = nn.BatchNorm1d(inp_dim)\n",
    "        self.n_d = n_d\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn(x)\n",
    "        x_a = self.first_step(x)[:,self.n_d:]\n",
    "        sparse_loss = torch.zeros(1).to(x.device)\n",
    "        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n",
    "        priors = torch.ones(x.shape).to(x.device)\n",
    "        for step in self.steps:\n",
    "            x_te,l = step(x,x_a,priors)\n",
    "            out += F.relu(x_te[:,:self.n_d])\n",
    "            x_a = x_te[:,self.n_d:]\n",
    "            sparse_loss += l\n",
    "        return self.fc(out),sparse_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
