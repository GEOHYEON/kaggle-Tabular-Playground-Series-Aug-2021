{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04c8d15",
   "metadata": {},
   "source": [
    "# TabNet CODE\n",
    "\n",
    "### Introduce TabNET\n",
    "---\n",
    "* 고스트 배치 정규화 (GBN)\n",
    "\n",
    "* Sparsemax\n",
    "\n",
    "* \n",
    "\n",
    "\n",
    "### code_source\n",
    "---\n",
    "* TabNet Torch code: https://ichi.pro/ko/pytorcheseo-tabnet-guhyeon-277727554318969\n",
    "* Sparsemax code: https://github.com/gokceneraslan/SparseMax.torch\n",
    "* paper: https://arxiv.org/pdf/1908.07442v4.pdf\n",
    "* pytorch-TabNet: https://wsshin.tistory.com/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3890dab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.8.11 (default, Aug  3 2021, 05:10:14) \\n[Clang 10.0.0 ]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ad7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bc66e85",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c6b0fdb55026>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import optuna\n",
    "\n",
    "import plotly as pl\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920599bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\") \n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69abb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = train_data.loc[:, 'f0':'f99']\n",
    "y_data = train_data.loc[:, 'loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21aa17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb61046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=train_test_split(x_data,\n",
    "                                                  y_data,\n",
    "                                                  test_size=0.2,   #전체 중 20%를 테스트용으로 분할\n",
    "                                                                   #나머지 80%는 훈련용\n",
    "                                                  shuffle=True,    #무작위로 섞어서 추출\n",
    "                                                  random_state=20) #무작위 추출 시 일정한 기준으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab5879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17dbaaba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a693e23c4fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGBN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class GBN(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp,vbs=128,momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "        \n",
    "    def forward(self,x):\n",
    "        chunk = torch.chunk(x,x.size(0)//self.vbs,0)\n",
    "        res = [self.bn(y) for y in chunk]\n",
    "        return torch.cat(res,0)\n",
    "\n",
    "# Attention Transformer\n",
    "class AttentionTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,d_a,inp_dim,relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_a,inp_dim)\n",
    "        self.bn = GBN(out_dim,vbs=vbs)\n",
    "        self.smax = Sparsemax()\n",
    "        self.r = relax\n",
    "        \n",
    "    #a:feature from previous decision step\n",
    "    def forward(self,a,priors): \n",
    "        a = self.bn(self.fc(a)) \n",
    "        mask = self.smax(a*priors) \n",
    "        priors =priors*(self.r-mask)  #updating the prior\n",
    "        return mask\n",
    "    \n",
    "# \n",
    "class GLU(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n",
    "        super().__init__()\n",
    "        if fc:\n",
    "            self.fc = fc\n",
    "        else:\n",
    "            self.fc = nn.Linear(inp_dim,out_dim*2)\n",
    "        self.bn = GBN(out_dim*2,vbs=vbs) \n",
    "        self.od = out_dim\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn(self.fc(x))\n",
    "        return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n",
    "        super().__init__()\n",
    "        first = True\n",
    "        self.shared = nn.ModuleList()\n",
    "        if shared:\n",
    "            self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n",
    "            first= False    \n",
    "            for fc in shared[1:]:\n",
    "                self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n",
    "        else:\n",
    "            self.shared = None\n",
    "        self.independ = nn.ModuleList()\n",
    "        if first:\n",
    "            self.independ.append(GLU(inp,out_dim,vbs=vbs))\n",
    "        for x in range(first, n_ind):\n",
    "            self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n",
    "        self.scale = torch.sqrt(torch.tensor([.5],device=device))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if self.shared:\n",
    "            x = self.shared[0](x)\n",
    "            for glu in self.shared[1:]:\n",
    "                x = torch.add(x, glu(x))\n",
    "                x = x*self.scale\n",
    "        for glu in self.independ:\n",
    "            x = torch.add(x, glu(x))\n",
    "            x = x*self.scale\n",
    "        return x\n",
    "\n",
    "class DecisionStep(nn.Module):\n",
    "    \n",
    "    def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax,vbs=128):\n",
    "        super().__init__()\n",
    "        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n",
    "        self.atten_tran =  AttentionTransformer(n_a,inp_dim,relax,vbs)\n",
    "        \n",
    "    def forward(self,x,a,priors):\n",
    "        mask = self.atten_tran(a,priors)\n",
    "        sparse_loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n",
    "        x = self.fea_tran(x*mask)\n",
    "        return x,sparse_loss\n",
    "\n",
    "class TabNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,\n",
    "                 n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n",
    "        super().__init__()\n",
    "        if n_shared>0:\n",
    "            self.shared = nn.ModuleList()\n",
    "            self.shared.append(nn.Linear(inp_dim,2*(n_d+n_a)))\n",
    "            for x in range(n_shared-1):\n",
    "                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n",
    "        else:\n",
    "            self.shared=None\n",
    "        self.first_step = FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind) \n",
    "        self.steps = nn.ModuleList()\n",
    "        for x in range(n_steps-1):\n",
    "            self.steps.append(DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n",
    "        self.fc = nn.Linear(n_d,final_out_dim)\n",
    "        self.bn = nn.BatchNorm1d(inp_dim)\n",
    "        self.n_d = n_d\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.bn(x)\n",
    "        x_a = self.first_step(x)[:,self.n_d:]\n",
    "        sparse_loss = torch.zeros(1).to(x.device)\n",
    "        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n",
    "        priors = torch.ones(x.shape).to(x.device)\n",
    "        for step in self.steps:\n",
    "            x_te,l = step(x,x_a,priors)\n",
    "            out += F.relu(x_te[:,:self.n_d])\n",
    "            x_a = x_te[:,self.n_d:]\n",
    "            sparse_loss += l\n",
    "        return self.fc(out),sparse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23912967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c330cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14acaee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
