{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9681ae",
   "metadata": {},
   "source": [
    "## XG-Boost\n",
    "\n",
    "---\n",
    "### parameter tuning\n",
    "* 일반 파라미터\n",
    "부스팅을 수행할 때 트리를 사용할지, 선형 모델을 사용할지 등을 고른다.\n",
    "* 부스터 파라미터\n",
    "선택한 부스터에 따라서 적용할 수 있는 파라미터 종류가 다르다.\n",
    "* 학습 과정 파라미터\n",
    "학습 시나리오를 결정한다.\n",
    "\n",
    "* 일반 파라미터\n",
    "    - booster [기본값 = gbtree]\n",
    "        어떤 부스터 구조를 쓸지 결정한다.\n",
    "        의사결정기반모형(gbtree), 선형모형(gblinear), dart가 있다.\n",
    "    - n_jobs\n",
    "        XGBoost를 실행하는 데 사용되는 병렬 스레드 수\n",
    "    - verbosity [기본값 = 1]\n",
    "        유효한 값은 0 (무음), 1 (경고), 2 (정보), 3 (디버그)\n",
    "        \n",
    "        \n",
    "* 부스터 파라미터\n",
    "    - gbtree Booster의 파라미터\n",
    "        - learning_rate [ 기본값 : 0.3 ]\n",
    "            learning rate가 높을수록 과적합 하기 쉽다.\n",
    "        - n_estimators [ 기본값 : 100 ]\n",
    "            생성할 weak learner의 수\n",
    "            learning_rate가 낮을 땐, n_estimators를 높여야 과적합이 방지된다.\n",
    "        - max_depth [ 기본값 : 6 ]\n",
    "            트리의 maximum depth이다.\n",
    "            적절한 값이 제시되어야 하고 보통 3-10 사이 값이 적용된다.\n",
    "            max_depth가 높을수록 모델의 복잡도가 커져 과적합 하기 쉽다.\n",
    "        - min_child_weight [ 기본값 : 1 ]\n",
    "            관측치에 대한 가중치 합의 최소를 말한다.\n",
    "            값이 높을수록 과적합이 방지된다.\n",
    "        - gamma [ 기본값 : 0 ]\n",
    "            리프노드의 추가분할을 결정할 최소손실 감소값이다.\n",
    "            해당값보다 손실이 크게 감소할 때 분리한다.\n",
    "            값이 높을수록 과적합이 방지된다.\n",
    "        - subsample [ 기본값 : 1 ]\n",
    "            weak learner가 학습에 사용하는 데이터 샘플링 비율이다.\n",
    "            보통 0.5 ~ 1 사용된다.\n",
    "            값이 낮을수록 과적합이 방지된다.\n",
    "        - colsample_bytree [ 기본값 : 1 ]\n",
    "            각 tree 별 사용된 feature의 퍼센테이지이다.\n",
    "            보통 0.5 ~ 1 사용된다.\n",
    "            값이 낮을수록 과적합이 방지된다.\n",
    "        - lambda [기본값 = 1, 별칭 : reg_lambda]\n",
    "            가중치에 대한 L2 Regularization 적용 값\n",
    "            피처 개수가 많을 때 적용을 검토\n",
    "            이 값이 클수록 과적합 감소 효과\n",
    "        - alpha [기본값 = 0, 별칭 : reg_alpha]\n",
    "            가중치에 대한 L1 Regularization 적용 값\n",
    "            피처 개수가 많을 때 적용을 검토\n",
    "            이 값이 클수록 과적합 감소 효과\n",
    "\n",
    "* 학습 과정 파라미터\n",
    "    - objective [ 기본값 : reg = squarederror ]\n",
    "        - reg : squarederror [제곱 손실이 있는 회귀 ]\n",
    "        - binary : logistic (binary-logistic classification)\n",
    "        [이항 분류 문제 로지스틱 회귀 모형으로 반환값이 클래스가 아니라 예측 확률]\n",
    "        - multi : softmax \n",
    "        다항 분류 문제의 경우 소프트맥스(Softmax)를 사용해서 분류하는데 반횐되는 값이 예측확률이 아니라 클래스임. 또한 num_class도 지정해야함.\n",
    "        - multi : softprob\n",
    "        클래스 범주에 속하는 예측확률을 반환함.\n",
    "        - count : poisson (count data poison regression) 등 다양하다.\n",
    "    - eval_metric : 모델의 평가 함수를 조정하는 함수다.\n",
    "    설정한 objective 별로 기본설정값이 지정되어 있다.\n",
    "        - rmse: root mean square error\n",
    "        - mae: mean absolute error\n",
    "        - logloss: negative log-likelihood\n",
    "        - error: Binary classification error rate (0.5 threshold)\n",
    "        - merror: Multiclass classification error rate\n",
    "        - mlogloss: Multiclass logloss\n",
    "        - auc: Area under the curve\n",
    "        map (mean average precision)등, 해당 데이터의 특성에 맞게 평가 함수를 조정한다.            \n",
    "    - seed [ 기본값 : 0 ]\n",
    "        재현가능하도록 난수를 고정시킴.\n",
    "\n",
    "* 민감하게 조정해야하는 것\n",
    "\n",
    "    - booster 모양\n",
    "    - eval_metric(평가함수) / objective(목적함수)\n",
    "    - eta \n",
    "    - L1 form (L1 레귤러라이제이션 폼이 L2보다 아웃라이어에 민감하다.)\n",
    "    - L2 form\n",
    "\n",
    "* 과적합 방지를 위해 조정해야하는 것\n",
    "\n",
    "    - learning rate 낮추기 → n_estimators은 높여야함\n",
    "    - max_depth 낮추기\n",
    "    - min_child_weight 높이기\n",
    "    - gamma 높이기\n",
    "    - subsample, colsample_bytree 낮추기\n",
    "\n",
    "\n",
    "---\n",
    "### 참고\n",
    "* https://www.kaggle.com/lifesailor/xgboost\n",
    "* https://wooono.tistory.com/97\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46b1cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "import optuna \n",
    "from optuna import Trial, visualization\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "#autogloun\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1763122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings( 'ignore' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "715863e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\") \n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c75374",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = train_data.loc[:, 'f0':'f99']\n",
    "y_data = train_data.loc[:, 'loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76522b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7e2797",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test=train_test_split(x_data,\n",
    "                                                  y_data,\n",
    "                                                  test_size=0.2,   #전체 중 20%를 테스트용으로 분할\n",
    "                                                                   #나머지 80%는 훈련용\n",
    "                                                  shuffle=True,    #무작위로 섞어서 추출\n",
    "                                                  random_state=20) #무작위 추출 시 일정한 기준으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd97547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# 모델 선언\n",
    "my_model = xgb.XGBRegressor(\n",
    "    verbosity = 2\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    n_estimators=100)\n",
    "\n",
    "# 모델 훈련\n",
    "my_model.fit(x_train, y_train, verbose=False)\n",
    "\n",
    "# 모델 예측\n",
    "y_test = my_model.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47002cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# 모델 선언\n",
    "my_model = xgb.XGBRegressor(\n",
    "    n_estimators = 3520,\n",
    "    max_depth = 11,\n",
    "    min_child_weight = 231,\n",
    "    gamma = 2,\n",
    "    colsample_bytree = 0.7,\n",
    "    reg_lambda = 0.014950936465569798,\n",
    "    alpha = 0.28520156840812494,\n",
    "    subsample = 0.6,\n",
    "    learning_rate=0.01)\n",
    "\n",
    "# 모델 훈련\n",
    "my_model.fit(x_train, y_train, verbose=False)\n",
    "\n",
    "# 모델 예측\n",
    "y_pred = my_model.predict(x_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d16a6bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.797709619810067\n"
     ]
    }
   ],
   "source": [
    "rms = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print(rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51cdf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objectiveXGB(trial: Trial, X, y, test):\n",
    "    param = {\n",
    "        \"n_estimators\" : trial.suggest_int('n_estimators', 100, 10000),\n",
    "        'max_depth':trial.suggest_int('max_depth', 5, 10),\n",
    "        'min_child_weight':trial.suggest_int('min_child_weight', 1, 300),\n",
    "        'gamma':trial.suggest_int('gamma', 1, 3),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.3, 0.1, 0.05, 0.01]),\n",
    "        'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree',0.5, 1, 0.1),\n",
    "        'tree_method': 'hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.6,0.7,0.8,1.0] ),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y.flatten(), test_size=0.1)\n",
    "    \n",
    "    y_train = y_train.reshape(-1, 1)\n",
    "    y_test  = y_test.reshape(-1, 1)\n",
    "\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    xgb_model = model.fit(X_train, y_train, verbose=False, eval_set=[(X_test, y_test)])\n",
    "    score = mean_squared_error(xgb_model.predict(X_test), y_test, squared=False)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8ef7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize',\n",
    "                            sampler=TPESampler())\n",
    "study.optimize(lambda trial : objectiveXGB(trial, X,  y, X_test), n_trials=50)\n",
    "print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\n",
    "\n",
    "best_param = study.best_trial.params\n",
    "xgbReg = train(xgb.XGBRegressor(**best_param, tree_method='gpu_hist', random_state=42, \n",
    "                                predictor='gpu_predictor', learning_rate=0.01, nthread=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe82ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee66bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna.visualization.plot_param_importances(study)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
